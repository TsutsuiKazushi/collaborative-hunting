{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from common.network import DuelingNetwork\n",
    "from common.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" speed \"\"\"\n",
    "speed = \"slow\" # \"slow\", \"equal\" or \"fast\"\n",
    "\n",
    "\"reward\"\n",
    "reward_p = \"share\" # \"indiv\" or \"share\"\n",
    "\n",
    "\"\"\" Epsilon \"\"\"\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\"\"\" seed \"\"\"\n",
    "seeds = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "names = [\"self_play\"]\n",
    "\n",
    "for seed in seeds:\n",
    "\n",
    "    \"\"\" participant \"\"\"\n",
    "    for name in names:\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "        \"\"\" divice \"\"\"\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(device)\n",
    "\n",
    "        \"\"\" Network \"\"\"\n",
    "        net_p1 = DuelingNetwork(18, 13).to(device)\n",
    "        net_p2 = DuelingNetwork(18, 13).to(device)\n",
    "        net_e = DuelingNetwork(18, 13).to(device)\n",
    "        \n",
    "        \"\"\" Environment \"\"\"\n",
    "        env_dir = os.path.join(os.pardir, \"c2ae\")\n",
    "        sys.path.append(env_dir)\n",
    "        from chase2_and_escape import Chase2AndEscape\n",
    "        \n",
    "        if speed == \"fast\":\n",
    "            speed_p = 3.6\n",
    "        elif speed == \"equal\":\n",
    "            speed_p = 3.0\n",
    "        elif speed == \"slow\":\n",
    "            speed_p = 2.4\n",
    "        \n",
    "        speed_e = 3\n",
    "        \n",
    "        if reward_p == \"indiv\":\n",
    "            share_reward = False\n",
    "        elif reward_p == \"share\":\n",
    "            share_reward = True\n",
    "        \n",
    "        max_step_episode = 300\n",
    "        env = Chase2AndEscape(speed_pursuer1=speed_p, speed_pursuer2=speed_p, speed_evader=speed_e, max_step=max_step_episode, reward_share=share_reward)\n",
    "        \n",
    "        \"\"\" Load \"\"\"\n",
    "        net_p1.load_state_dict(torch.load(\"../model/c2ae/reward_\" + reward_p + \"/p1_\" +  str (speed_p) + \".pth\")) \n",
    "        net_p2.load_state_dict(torch.load(\"../model/c2ae/reward_\" + reward_p + \"/p2_\" +  str (speed_p) + \".pth\")) \n",
    "        net_e.load_state_dict(torch.load(\"../model/c2ae/reward_\" + reward_p + \"/e_\" +  str (speed_p) + \".pth\")) \n",
    "\n",
    "        \"\"\" No. of episodes \"\"\"\n",
    "        num_episodes_test = 100\n",
    "\n",
    "        \"\"\" Simulation \"\"\"\n",
    "        rep_v_list = []\n",
    "        rep_a_list = []\n",
    "        q_list = []\n",
    "        pos_list = []\n",
    "\n",
    "        for i in range(num_episodes_test):\n",
    "\n",
    "            pursuer1_rep_v_episode = []\n",
    "            pursuer2_rep_v_episode = []\n",
    "            evader_rep_v_episode = []\n",
    "            pursuer1_rep_a_episode = []\n",
    "            pursuer2_rep_a_episode = []\n",
    "            evader_rep_a_episode = []\n",
    "            pursuer1_q_episode = []\n",
    "            pursuer2_q_episode = []\n",
    "            evader_q_episode = []\n",
    "            pursuer1_pos_episode = []\n",
    "            pursuer2_pos_episode = []\n",
    "            evader_pos_episode = []\n",
    "\n",
    "            obs_p1, obs_p2, obs_e = env.reset()\n",
    "            obs_p1, obs_p2, obs_e = torch.Tensor(obs_p1), torch.Tensor(obs_p2), torch.Tensor(obs_e)\n",
    "            done = False\n",
    "            step_episode = 0\n",
    "\n",
    "            while not done:\n",
    "\n",
    "                feature_p1 = net_p1.forward_com(obs_p1.float().to(device))\n",
    "                feature_v_p1 = torch.relu(torch.matmul(feature_p1, net_p1.fc_state[0].weight.T) + net_p1.fc_state[0].bias)\n",
    "                feature_a_p1 = torch.relu(torch.matmul(feature_p1, net_p1.fc_advantage[0].weight.T) + net_p1.fc_advantage[0].bias)\n",
    "\n",
    "                feature_p2 = net_p2.forward_com(obs_p2.float().to(device))\n",
    "                feature_v_p2 = torch.relu(torch.matmul(feature_p2, net_p2.fc_state[0].weight.T) + net_p2.fc_state[0].bias)\n",
    "                feature_a_p2 = torch.relu(torch.matmul(feature_p2, net_p2.fc_advantage[0].weight.T) + net_p2.fc_advantage[0].bias)\n",
    "                \n",
    "                feature_e = net_e.forward_com(obs_e.float().to(device))\n",
    "                feature_v_e = torch.relu(torch.matmul(feature_e, net_e.fc_state[0].weight.T) + net_e.fc_state[0].bias)\n",
    "                feature_a_e = torch.relu(torch.matmul(feature_e, net_e.fc_advantage[0].weight.T) + net_e.fc_advantage[0].bias)\n",
    "\n",
    "                q_p1 = net_p1.forward(obs_p1.float().to(device))\n",
    "                q_p2 = net_p2.forward(obs_p2.float().to(device))\n",
    "                q_e = net_e.forward(obs_e.float().to(device))\n",
    "\n",
    "                action_p1 = net_p1.act(obs_p1.float().to(device), epsilon)\n",
    "                action_p2 = net_p2.act(obs_p2.float().to(device), epsilon)\n",
    "                action_e = net_e.act(obs_e.float().to(device), epsilon)\n",
    "\n",
    "                next_obs_p1, next_obs_p2, next_obs_e, reward_p1, reward_p2, reward_e, done = env.step(action_p1, action_p2, action_e, step_episode)\n",
    "                next_obs_p1, next_obs_p2, next_obs_e = torch.Tensor(next_obs_p1), torch.Tensor(next_obs_p2), torch.Tensor(next_obs_e)    \n",
    "\n",
    "                obs_p1 = next_obs_p1\n",
    "                obs_p2 = next_obs_p2\n",
    "                obs_e = next_obs_e\n",
    "                step_episode += 1\n",
    "\n",
    "                pos_p1 = env.pos_p1\n",
    "                pos_p2 = env.pos_p2\n",
    "                pos_e = env.pos_e\n",
    "\n",
    "                pursuer1_rep_v_episode.append(np.array(feature_v_p1.detach().numpy()))\n",
    "                pursuer1_rep_a_episode.append(np.array(feature_a_p1.detach().numpy()))\n",
    "                pursuer2_rep_v_episode.append(np.array(feature_v_p2.detach().numpy()))\n",
    "                pursuer2_rep_a_episode.append(np.array(feature_a_p2.detach().numpy()))\n",
    "                evader_rep_v_episode.append(np.array(feature_v_e.detach().numpy()))\n",
    "                evader_rep_a_episode.append(np.array(feature_a_e.detach().numpy()))\n",
    "                pursuer1_q_episode.append(np.array(q_p1.detach().numpy()))\n",
    "                pursuer2_q_episode.append(np.array(q_p2.detach().numpy()))\n",
    "                evader_q_episode.append(np.array(q_e.detach().numpy()))\n",
    "                pursuer1_pos_episode.append(np.array(pos_p1))\n",
    "                pursuer2_pos_episode.append(np.array(pos_p2))\n",
    "                evader_pos_episode.append(np.array(pos_e))\n",
    "\n",
    "            rep_v_episode = []\n",
    "            rep_v_episode.append(evader_rep_v_episode)\n",
    "            rep_v_episode.append(pursuer1_rep_v_episode)\n",
    "            rep_v_episode.append(pursuer2_rep_v_episode)\n",
    "            rep_v_list.append(rep_v_episode)\n",
    "\n",
    "            rep_a_episode = []\n",
    "            rep_a_episode.append(evader_rep_a_episode)\n",
    "            rep_a_episode.append(pursuer1_rep_a_episode)\n",
    "            rep_a_episode.append(pursuer2_rep_a_episode)\n",
    "            rep_a_list.append(rep_a_episode)\n",
    "            \n",
    "            q_episode = []\n",
    "            q_episode.append(evader_q_episode)\n",
    "            q_episode.append(pursuer1_q_episode)\n",
    "            q_episode.append(pursuer2_q_episode)\n",
    "            q_list.append(q_episode)\n",
    "\n",
    "            pos_episode = []\n",
    "            pos_episode.append(evader_pos_episode)\n",
    "            pos_episode.append(pursuer1_pos_episode)\n",
    "            pos_episode.append(pursuer2_pos_episode)\n",
    "            pos_list.append(pos_episode)\n",
    "            \n",
    "        \"\"\" Save \"\"\"\n",
    "        save_dir = \"self_play_results\"\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        if epsilon==0:\n",
    "            name_policy = \"greedy\"\n",
    "        else:\n",
    "            name_policy = \"epsilon_greedy\"\n",
    "        \n",
    "        file_path = os.path.join(save_dir, \"results_2on1_\" + name + \"_\" + reward_p + \"_\" +  speed + \"_\" + name_policy + \"_seed_\" + str(seed) + \".npz\")\n",
    "        np.savez(file_path, pos=pos_list, q=q_list, rep_v=rep_v_list, rep_a=rep_a_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
